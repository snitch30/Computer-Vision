# -*- coding: utf-8 -*-

Automatically generated by Colaboratory.


# Initial Setup
This Jupyter Notebook was successfully run in Google Colab, and input files are located in my personal google drive as well as in files submitted along with. This jupyter notebook uses the files in my google drive. Please change the path accordingly to run.

### *Note:* 
Google Colab has been used to do this assignment due to lack of a personal GPU machine with CUDA.
"""

from google.colab import drive
drive.mount('/content/drive')

"""> Importing Libraries and setting path for necessary files."""

from __future__ import print_function
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import datasets, transforms
import matplotlib.pyplot as plt
import pdb
import numpy as np
import pandas as pd
import os

path='drive/My Drive/computer_vision/assignment_02/'
incsv_path=path+'SIVARAMT@TCD.IE_in.csv'
netcsv_path=path+'SIVARAMT@TCD.IE_network.csv'
sigma_path=path+'SIVARAMT@TCD.IE_sigma.csv'
dncnn_path=path+'DnCNN/'
img_path=path+'SIVARAMT@TCD.IE_007.png'

"""# MNIST classification (20 marks)

## You are provided with code mnist.py that trains a Convolutional Neural Network on MNIST dataset. You are required to modify this file.

### a) In the file mnist.py identify where the Stochastic Gradient Descent optimizer is created. Train the default CNN architecture by choosing appropriate parameter values for:

 i) Learning rate – explain what happens when you use too large or too small value and
explain why it is happening.

 ii) Weight decay – explain what happens when you use too large or too small regularization
and explain why it is happening.

> i) Learning rate – explain what happens when you use too large or too small value and explain why it is happening


Learning rate determines how fast or slow the convergence should be. It influences the amount that the weights are updated. Large values of learning rate means large change in weights. It shouldn’t be too large as we may end up diverging and training becomes unstable. It shouldn’t be too small as we may take way longer time (a lot of epochs) in converging and sometimes doesn’t converge. 

Learning rate around 0.1 or more led to an accuracy around 10% or less. On trying different values it was found that Learning Rate as 0.01 produced the best accuracy being 98%.

> ii) Weight decay – explain what happens when you use too large or too small regularization and explain why it is happening.

Weight Decay helps in how fast or slow the weight decays to zero.  Weight decay helps in generalising and helps tackle overfitting by penalising weight, by subtracting 
`weight_decay x weight` from `weight`. The problem with this being too high is it might make our model underfit. If it is too small we might have to stop the training earlier. One of the benefits with weight decay/weight regularisation is it helps us tackle what is known as _exploding gradient problem_, where gradients accumulate leading to large values which makes the network unstable and weights end up becoming so large.

Weight Decay around 0.1 or more with Learning Rate equal to 0.01, led to an accuracy around 96%. On trying different values it was found that Weight Decay as 0.001 produced the best accuracy being 98%.

More changes led to an accuracy of 98% with a learning rate of 0.01 and a weight decay of 0.001

We can see the results with an Average Loss of 0.0523 and an Accuracy of 9834/10000 (98%)
"""

# Training hyperparameters
epochs = 1
batch_size = 16
learning_rate = 0.01 # TODO
momentum = 0.9
weight_decay = 0.001 # TODO
log_interval = 20

class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=16, 
                               kernel_size=5, stride=1, padding=0)
        self.maxpool = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(in_channels=16, out_channels=32, 
                               kernel_size=3, stride=1, padding=0)
        self.fc1 = nn.Linear(in_features=800, out_features=128)
        self.fc2 = nn.Linear(in_features=128, out_features=10)

        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')
        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')
        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')
        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.maxpool(x)
        x = x.view(-1, 800)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

class CNN2(nn.Module):
    def __init__(self):
        super(CNN2, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, 
                               kernel_size=5, stride=1, padding=0)
        self.maxpool = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, 
                               kernel_size=5, stride=1, padding=0)
        self.fc1 = nn.Linear(in_features=1024, out_features=256)
        self.fc2 = nn.Linear(in_features=256, out_features=10)

        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='relu')
        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='relu')
        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='relu')
        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.maxpool(x)
        # print(x.shape)
        x = x.view(-1, 1024)
        # print(x.shape)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

class CNN3(nn.Module):
    def __init__(self):
        super(CNN3, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=1, out_channels=32, 
                               kernel_size=5, stride=1, padding=0)
        self.maxpool = nn.MaxPool2d(2)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, 
                               kernel_size=5, stride=1, padding=0)
        self.fc1 = nn.Linear(in_features=1024, out_features=256)
        self.dropout = nn.Dropout(0.2)
        self.fc2 = nn.Linear(in_features=256, out_features=10)
        self.dropout = nn.Dropout(0.2)

        nn.init.kaiming_normal_(self.conv1.weight, nonlinearity='leaky_relu')
        nn.init.kaiming_normal_(self.conv2.weight, nonlinearity='leaky_relu')
        nn.init.kaiming_normal_(self.fc1.weight, nonlinearity='leaky_relu')
        nn.init.kaiming_normal_(self.fc2.weight, nonlinearity='linear')
        # TODO        

    def forward(self, x):
        x = self.conv1(x)
        x = F.relu(x)
        x = self.maxpool(x)
        x = self.conv2(x)
        x = F.relu(x)
        x = self.maxpool(x)
        # print(x.shape)
        x = x.view(-1, 1024)
        # print(x.shape)
        x = self.fc1(x)
        x = F.relu(x)
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

        # def __init__(self):
        # super(mnist_model, self).__init__()
        # self.feats = nn.Sequential(
        #     nn.Conv2d(1, 32, 3, 1, 1),
        #     nn.MaxPool2d(2, 2),
        #     nn.ReLU(True),
        #     nn.BatchNorm2d(32),

        #     nn.Conv2d(32, 64, 3,  1, 1),
        #     nn.ReLU(True),
        #     nn.BatchNorm2d(64),

        #     nn.Conv2d(64, 64, 3,  1, 1),
        #     nn.MaxPool2d(2, 2),
        #     nn.ReLU(True),
        #     nn.BatchNorm2d(64),

        #     nn.Conv2d(64, 128, 3, 1, 1),
        #     nn.ReLU(True),
        #     nn.BatchNorm2d(128)
        # )

        # self.classifier = nn.Conv2d(128, 10, 1)
        # self.avgpool = nn.AvgPool2d(6, 6)
        # self.dropout = nn.Dropout(0.5)

def plot_data(data, label, text):
    fig = plt.figure()
    for i in range(6):
        plt.subplot(2,3,i+1)
        plt.tight_layout()
        plt.imshow(data[i][0], cmap='gray', interpolation='none')
        plt.title(text + ": {}".format(label[i]))
        plt.xticks([])
        plt.yticks([])
    plt.show()

def predict_batch(model, device, test_loader):
    examples = enumerate(test_loader)
    model.eval()
    with torch.no_grad():
        batch_idx, (data, target) = next(examples)
        data, target = data.to(device), target.to(device)
        output = model(data)
        pred = output.cpu().data.max(1, keepdim=True)[1] # get the index of the max log-probability
        pred = pred.numpy()
    return data.cpu().data.numpy(), target.cpu().data.numpy(), pred

def plot_graph(train_x, train_y, test_x, test_y, ylabel=''):
    fig = plt.figure()
    plt.plot(train_x, train_y, color='blue')
    plt.plot(test_x, test_y, color='red')
    plt.legend(['Train', 'Test'], loc='upper right')
    plt.xlabel('number of training examples seen')
    plt.ylabel(ylabel)
    plt.grid()
    plt.show()

def train(model, device, train_loader, optimizer, epoch, losses=[], counter=[], errors=[]):
    model.train()
    correct=0
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)
        optimizer.zero_grad()
        output = model(data)
        loss = F.nll_loss(output, target)
        loss.backward()
        optimizer.step()
        if batch_idx % log_interval == 0:
            print('Train Epoch: {} [{}/{} ({:.0f}%)]\tLoss: {:.6f}'.format(
                epoch, batch_idx * len(data), len(train_loader.dataset),
                100. * batch_idx / len(train_loader), loss.item()))
            losses.append(loss.item())
            counter.append((batch_idx*batch_size) + ((epoch-1)*len(train_loader.dataset)))
        pred = output.max(1, keepdim=True)[1]
        correct += pred.eq(target.view_as(pred)).sum().item()
    errors.append(100. * (1 - correct / len(train_loader.dataset)))

def test(model, device, test_loader, losses=[], errors=[]):
    model.eval()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss
            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader.dataset)
    print('\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\n'.format(
        test_loss, correct, len(test_loader.dataset),
        100. * correct / len(test_loader.dataset)))
    losses.append(test_loss)
    errors.append(100. *  (1 - correct / len(test_loader.dataset)))

"""> MNIST with default CNN architectre called using `CNN()` function."""

def main():
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # data transformation
    train_data = datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ]))
    test_data = datasets.MNIST('../data', train=False, 
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ]))
        # data loaders
    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)

	# extract and plot random samples of data
    #examples = enumerate(test_loader)
    #batch_idx, (data, target) = next(examples)
    #plot_data(data, target, 'Ground truth')
    
	
        # model creation
    model = CNN().to(device)
    # optimizer creation
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)

    # lists for saving history
    train_losses = []
    train_counter = []
    test_losses = []
    test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]
    train_errors = []
    test_errors = []
    error_counter = [i*len(train_loader.dataset) for i in range(epochs)]
    
        # test of randomly initialized model
    test(model, device, test_loader, losses=test_losses)

    # global training and testing loop
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, optimizer, epoch, losses=train_losses, counter=train_counter, errors=train_errors)
        test(model, device, test_loader, losses=test_losses, errors=test_errors)
		
            # plotting training history
    plot_graph(train_counter, train_losses, test_counter, test_losses, ylabel='negative log likelihood loss')
    plot_graph(error_counter, train_errors, error_counter, test_errors, ylabel='error (%)')
	
    # extract and plot random samples of data with predicted labels
    data, _, pred = predict_batch(model, device, test_loader)
    plot_data(data, pred, 'Predicted')
	
if __name__ == '__main__':
    main()

"""B- Correct the mistakes in CNN2 and train it on MNIST train set. Desired architecture of CNN2 is
displayed on given diagam, CONV KxK, N represents N features extracted by KxK filters, FC N represent fully-connected layer with N nodes. Set the padding so each convolution preserves input feature size.

> MNIST with desired CNN architectre given in the diagram called using `CNN2()` function.
"""

def main():
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

    # data transformation
    train_data = datasets.MNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ]))
    test_data = datasets.MNIST('../data', train=False, 
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((0.1307,), (0.3081,))
                   ]))
        # data loaders
    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)

	# extract and plot random samples of data
    #examples = enumerate(test_loader)
    #batch_idx, (data, target) = next(examples)
    #plot_data(data, target, 'Ground truth')
    
	
        # model creation
    model = CNN2().to(device)
    # optimizer creation
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)

    # lists for saving history
    train_losses = []
    train_counter = []
    test_losses = []
    test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]
    train_errors = []
    test_errors = []
    error_counter = [i*len(train_loader.dataset) for i in range(epochs)]
    
        # test of randomly initialized model
    test(model, device, test_loader, losses=test_losses)

    # global training and testing loop
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, optimizer, epoch, losses=train_losses, counter=train_counter, errors=train_errors)
        test(model, device, test_loader, losses=test_losses, errors=test_errors)
		
            # plotting training history
    plot_graph(train_counter, train_losses, test_counter, test_losses, ylabel='negative log likelihood loss')
    plot_graph(error_counter, train_errors, error_counter, test_errors, ylabel='error (%)')
	
    # extract and plot random samples of data with predicted labels
    data, _, pred = predict_batch(model, device, test_loader)
    plot_data(data, pred, 'Predicted')
	
if __name__ == '__main__':
    main()

"""C- Change dataset to fashion MNIST (https://research.zalando.com/welcome/mission/research- projects/fashion-mnist/, hint: take a look at torchvision.datasets), estimate the dataset mean and standard deviation and use it to normalize the data in the data loader.

> Function that calculates mean and standard deviation which is used for normalising.
> The mean and standard deviation are used _in the next cell_ by calling this function, where the dataset is also changed to Fasion-MNIST as asked.
"""

# def mean_std_func(loader, train_data):
  #   ## Calculating Mean
  #   mean=np.zeros((28,28))
  #   for i in range(60000):
  #     temp=train_data[i][0][0].numpy()
  #     mean=mean+temp
  #   mean=mean/60000
  #   mean2=np.sum(mean)/784
  #   ## Calculating Standard Deviation
  #   mean = 0.0
  #   for images, _ in loader:
  #     batch_samples = images.size(0) 
  #     images = images.view(batch_samples, images.size(1), -1)
  #     mean += images.mean(2).sum(0)
  #   mean = mean / len(loader.dataset) 
  #   var=0
  #   for images, _ in loader:
  #     batch_samples = images.size(0) 
  #     images = images.view(batch_samples, images.size(1), -1) 
  #     var += ((images - mean.unsqueeze(1))**2).sum([0,2])
  #   std = torch.sqrt(var / (len(loader.dataset)*224*224))
  #   std=std.numpy()[0]*10
  #   return mean2,std


def mean_std_func(loader):
  samples = enumerate(loader)
  _, (loader_data, _) = next(samples)
  mean = loader_data.mean()
  stddev = loader_data.std()
  return mean, stddev

def main():
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

# data transformation
    train_data = datasets.FashionMNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                   ]))
    test_data = datasets.FashionMNIST('../data', train=False, 
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                   ]))
    # data loaders for normalization
    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)

    # Getting mean and stddev
    train_mean, train_stddev = mean_std_func(train_loader)

    # Re-loading numerical data, normalized
    train_data = datasets.FashionMNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((train_mean,), (train_stddev,))
                   ]))
    test_data = datasets.FashionMNIST('../data', train=False, 
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((train_mean,), (train_stddev,))
                   ]))
    # data loaders for normalization
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)

	# extract and plot random samples of data
    #examples = enumerate(test_loader)
    #batch_idx, (data, target) = next(examples)
    #plot_data(data, target, 'Ground truth')

    # model creation
    model = CNN2().to(device)
    # optimizer creation
    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
    

    # lists for saving history
    train_losses = []
    train_counter = []
    test_losses = []
    test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]
    train_errors = []
    test_errors = []
    error_counter = [i*len(train_loader.dataset) for i in range(epochs)]

    # test of randomly initialized model
    test(model, device, test_loader, losses=test_losses)

    # global training and testing loop
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, optimizer, epoch, losses=train_losses, counter=train_counter, errors=train_errors)
        test(model, device, test_loader, losses=test_losses, errors=test_errors)
		
    # plotting training history
    plot_graph(train_counter, train_losses, test_counter, test_losses, ylabel='negative log likelihood loss')
    plot_graph(error_counter, train_errors, error_counter, test_errors, ylabel='error (%)')
	
    # extract and plot random samples of data with predicted labels
    data, _, pred = predict_batch(model, device, test_loader)
    plot_data(data, pred, 'Predicted')
	
if __name__ == '__main__':
    main()
    epochs = 5

"""D- Design CNN3 with additional regularization of your choosing. Explain benefits of such regularization and report its accurracy on fashion MNIST and its relative improvement over CNN2.

> Fasion MNIST with custom CNN architectre with additional regularization of my choice, called using `CNN3()` function.

> Benefits of my `CNN3` architecture.

The `CNN3` model has a similar convolution layer to that of `CNN2`, however it uses two layers dropout as a regularisation technique. Moreover unlike `CNN2`, it doesn't use `ReLU` function at the end of each convolution, but uses `LeakyReLU` instead. Apart from this the optimiser used in `CNN2` was `SGD` whereas `CNN3` implemented `AdaDelta`.

Dropout Regularisation helps in reducing overfitting. This is done by randomly dropping nodes. This also ends up making the training a little noisier, which makes the model more robust to such noise while testing. 

`LeakyReLU` helps overcome dying ReLU problem, where a neuron gets stuck in the negative side and always outputs 0 and ends up not being able to recover from it. Over time we would have a lot of neurons ending up giving 0, which makes the network's performance very poor. This is solved by LeakyReLU by providing a small value over the negative range instead of zeros like ReLU.

`AdaGrad` was used as an optimzer.  This is a type of adaptive gradient which allows the learning rate to adapt based on parameters.

We see the combination of these changes provide a better result.

CNN2: Accuracy: 8526/10000 (85%) and Average loss: 0.3957
CNN3: Accuracy: 9036/10000 (90%) and Average loss: 0.2746
"""

def main():
    use_cuda = torch.cuda.is_available()
    device = torch.device("cuda" if use_cuda else "cpu")

# data transformation
    train_data = datasets.FashionMNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                   ]))
    test_data = datasets.FashionMNIST('../data', train=False, 
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                   ]))
    # data loaders for normalization
    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)

    # Getting mean and stddev
    train_mean, train_stddev = mean_std_func(train_loader)

    # Re-loading numerical data, normalized
    train_data = datasets.FashionMNIST('../data', train=True, download=True,
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((train_mean,), (train_stddev,))
                   ]))
    test_data = datasets.FashionMNIST('../data', train=False, 
                   transform=transforms.Compose([
                       transforms.ToTensor(),
                       transforms.Normalize((train_mean,), (train_stddev,))
                   ]))
    # data loaders for normalization
    train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True, **kwargs)
    test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False, **kwargs)

	# extract and plot random samples of data
    #examples = enumerate(test_loader)
    #batch_idx, (data, target) = next(examples)
    #plot_data(data, target, 'Ground truth')

    # model creation
    model = CNN3().to(device)
    # optimizer creation
    # optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)
    optimizer = optim.Adagrad(model.parameters(), lr=0.01, lr_decay=0, weight_decay=0, initial_accumulator_value=0, eps=1e-10)
    # lists for saving history
    train_losses = []
    train_counter = []
    test_losses = []
    test_counter = [i*len(train_loader.dataset) for i in range(epochs + 1)]
    train_errors = []
    test_errors = []
    error_counter = [i*len(train_loader.dataset) for i in range(epochs)]

    # test of randomly initialized model
    test(model, device, test_loader, losses=test_losses)

    # global training and testing loop
    for epoch in range(1, epochs + 1):
        train(model, device, train_loader, optimizer, epoch, losses=train_losses, counter=train_counter, errors=train_errors)
        test(model, device, test_loader, losses=test_losses, errors=test_errors)
		
    # plotting training history
    plot_graph(train_counter, train_losses, test_counter, test_losses, ylabel='negative log likelihood loss')
    plot_graph(error_counter, train_errors, error_counter, test_errors, ylabel='error (%)')
	
    # extract and plot random samples of data with predicted labels
    data, _, pred = predict_batch(model, device, test_loader)
    plot_data(data, pred, 'Predicted')
	
if __name__ == '__main__':
    main()

"""E- Download file YourEmail_in.csv from the course drive. File contains 5x5 array of integers X. Manually compute valid convolution of w and X and submit output in file named YourEmail_out.csv (note the difference between convolution and cross-correlation, see https://en.wikipedia.org/wiki/Convolution)."""

incsv=pd.read_csv(incsv_path,header=None)
incsv=np.array(incsv)
w=[[1,0,1],[0,0,0],[-1,0,-1]]
conv_w=np.flipud(np.fliplr(w)) 
y,x=np.shape(incsv)
m,n=np.shape(conv_w)
final_conv=np.zeros((y-m+1,x-n+1))
### The below three lines perfoms 2D convolution without padding.
for i in range(y-m+1):
  for j in range(x-m+1):
    final_conv[i][j] = np.sum(incsv[i:i+m, j:j+m]*conv_w)
pd.DataFrame(final_conv).to_csv('SIVARAMT@TCD.IE_out.csv',index=False,header=False)
print('Convoluted Output \n')
print(final_conv)

"""> Cross correlation generally is sliding a filter/kernel across the image while multiplying and summing, whereas in convolution you slide a flipped filter/kernel across the image while multiplying and summing.
> This can also be infered from the formula used in calculating both. As in convolution the function takes place over infinity to -infinity (depicted by the negative sign) opposite to that of the image/data, whereas in cross correlation the function takes place over -infinity to infinity (depicted by the positive sign) same that of the image/data.

F- Download file YourEmail_network.csv from the course drive. File contains network definiton in the following format:

i) Convolution, N, K – convolution layer with N feature maps and KxK filters

ii) Maxpool, K – max pooling layer KxK pooling window

iii) Fully-connected, N – fully connected layer with N neurons

All convolutions use same padding that preserves input feature size. All convolutional and fully-connected layers use bias. The network is applied on 32x32 RGB images (3 input channels). Calculate the number of parameters and the number of FLOPS of this network and submit them in YourEmail_params.csv each of the 2 values in separate row (number of parameters in the first row).
"""

network=pd.read_csv(netcsv_path,header=None)
print(network)

inp=3
conv1_out=network[1][0]
conv2_in=conv1_out
conv2_out=network[1][1]
conv3_in=conv2_out
conv3_out=network[1][3]
mp1=network[1][2]
mp2=network[1][4]
fc1_in=conv3_out*(32*32)/(mp1*mp1*mp2*mp2)
fc1_out=network[1][5]
fc2_in=fc1_out
fc2_out=network[1][6]
k1=7
k2=3
k3=3

def conv_param(n,m,k):
  conv=n*m*k*k+m
  return(conv)

def fc_param(n,m):
  fc=n*m+m
  return(fc)

def conv_flops(n,m,k,x):
  conv=n*m*k*k*x*x
  return(conv)

def fc_flops(n,m):
  fc=n*m
  return(fc)

total_param=conv_param(inp,conv1_out,k1)+conv_param(conv2_in,conv2_out,k2)+conv_param(conv3_in,conv3_out,k3)+fc_param(fc1_in,fc1_out)+fc_param(fc2_in,fc2_out)
total_flops=conv_flops(inp,conv1_out,k1,32)+conv_flops(conv2_in,conv2_out,k2,32)+conv_flops(conv3_in,conv3_out,k3,16)+fc_flops(fc1_in,fc1_out)+fc_flops(fc2_in,fc2_out)
print('\nTotal Parameters = %d \nTotal Flops = %d'%(total_param,total_flops))
params=[total_param,total_flops]
pd.DataFrame(params).to_csv('SIVARAMT@TCD.IE_params.csv',index=False,header=False)

"""## REFERENCES

1. https://machinelearningmastery.com/exploding-gradients-in-neural-networks/
2. https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484
3. https://stats.stackexchange.com/questions/29130/difference-between-neural-net-weight-decay-and-learning-rate
4. https://towardsdatascience.com/the-vanishing-gradient-problem-69bf08b15484
5. https://machinelearningmastery.com/exploding-gradients-in-neural-networks/
"""